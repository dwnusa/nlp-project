{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupSequence(lst): \n",
    "    res = [[lst[0]]] \n",
    "  \n",
    "    for i in range(1, len(lst)): \n",
    "        if lst[i-1][0]+1 == lst[i][0]: \n",
    "            res[-1].append(lst[i]) \n",
    "        else: \n",
    "            res.append([lst[i]]) \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[8, 'A'], [9, 'B'], [10, 'C']], [[7, 'D'], [8, 'E']], [[1, 'F'], [2, 'G'], [3, 'H']]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Driver program  \n",
    "l = [[8,\"A\"], [9,\"B\"], [10,\"C\"], [7,\"D\"], [8,\"E\"], [1,\"F\"], [2,\"G\"], [3,\"H\"]] \n",
    "# print([[l[0]]])\n",
    "print(groupSequence(l)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.txt', '2.txt', '3.txt', '4.txt', '5.txt', '6.txt', '7.txt', '8.txt', '9.txt', '10.txt', '11.txt', '12.txt', '13.txt', '14.txt', '15.txt', '16.txt', '17.txt', '18.txt', '19.txt', '20.txt', '21.txt', '22.txt', '23.txt', '24.txt', '25.txt', '26.txt', '27.txt', '28.txt', '29.txt', '30.txt', '31.txt', '32.txt', '33.txt', '34.txt', '35.txt', '36.txt', '37.txt', '38.txt', '39.txt', '40.txt', '41.txt', '42.txt', '43.txt', '44.txt', '45.txt', '46.txt', '47.txt', '48.txt', '49.txt', '50.txt']\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir('./50_reports(NLP_IR)')\n",
    "data_items = list(filter(lambda x: re.split('\\.',x)[0].isdigit(), arr))\n",
    "sorted_data_items = natural_sort(data_items)\n",
    "print(sorted_data_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: final report examination:  dx chest port line/tube plcmt 1 exam  indication:  ___ year old woman with likely ileus after cystectomy  // ngt placement confirmation      ngt placement confirmation  impression:   no previous images.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('final', 'JJ'), ('report', 'NN'), ('examination', 'NN'), (':', ':'), ('dx', 'NN'), ('chest', 'JJS'), ('port', 'NN'), ('line/tube', 'NN'), ('plcmt', 'VBD'), ('1', 'CD'), ('exam', 'JJ'), ('indication', 'NN'), (':', ':'), ('___', 'JJ'), ('year', 'NN'), ('old', 'JJ'), ('woman', 'NN'), ('with', 'IN'), ('likely', 'JJ'), ('ileus', 'NN'), ('after', 'IN'), ('cystectomy', 'NN'), ('//', 'JJ'), ('ngt', 'JJ'), ('placement', 'NN'), ('confirmation', 'NN'), ('ngt', 'FW'), ('placement', 'NN'), ('confirmation', 'NN'), ('impression', 'NN'), (':', ':'), ('no', 'DT'), ('previous', 'JJ'), ('images', 'NNS'), ('.', '.')]\n",
      "JNN-NJNN--JN-JNJN-JN-NJJNN-NNN-DJN-\n",
      "8 ['JNN', 'JNN', 'JN', 'JN', 'JN', 'JN', 'JJNN', 'JN']\n",
      "\n",
      "\n",
      "1: nasogastric tube extends to the mid body of the stomach, be for coiling on itself so that the tip lies close to the esophagogastric junction.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('nasogastric', 'JJ'), ('tube', 'NN'), ('extends', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('mid', 'NN'), ('body', 'NN'), ('of', 'IN'), ('the', 'DT'), ('stomach', 'NN'), (',', ','), ('be', 'VB'), ('for', 'IN'), ('coiling', 'VBG'), ('on', 'IN'), ('itself', 'PRP'), ('so', 'RB'), ('that', 'IN'), ('the', 'DT'), ('tip', 'NN'), ('lies', 'VBZ'), ('close', 'RB'), ('to', 'TO'), ('the', 'DT'), ('esophagogastric', 'JJ'), ('junction', 'NN'), ('.', '.')]\n",
      "JN--DNN-DN--------DN---DJN-\n",
      "2 ['JN', 'JN']\n",
      "\n",
      "\n",
      "2: for more optimal positioning, the to would have to be pulled back almost 10 cm and then hopefully redirected toward the lower stomach.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('for', 'IN'), ('more', 'JJR'), ('optimal', 'JJ'), ('positioning', 'NN'), (',', ','), ('the', 'DT'), ('to', 'TO'), ('would', 'MD'), ('have', 'VB'), ('to', 'TO'), ('be', 'VB'), ('pulled', 'VBN'), ('back', 'RB'), ('almost', 'RB'), ('10', 'CD'), ('cm', 'NNS'), ('and', 'CC'), ('then', 'RB'), ('hopefully', 'RB'), ('redirected', 'VBN'), ('toward', 'IN'), ('the', 'DT'), ('lower', 'JJR'), ('stomach', 'NN'), ('.', '.')]\n",
      "-JJN-D---------NC----DJN-\n",
      "2 ['JJN', 'JN']\n",
      "\n",
      "\n",
      "3: cardiac silhouette is within normal limits and there is no vascular congestion, pleural effusion, or acute focal pneumonia.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('cardiac', 'JJ'), ('silhouette', 'NN'), ('is', 'VBZ'), ('within', 'IN'), ('normal', 'JJ'), ('limits', 'NNS'), ('and', 'CC'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('vascular', 'JJ'), ('congestion', 'NN'), (',', ','), ('pleural', 'JJ'), ('effusion', 'NN'), (',', ','), ('or', 'CC'), ('acute', 'JJ'), ('focal', 'JJ'), ('pneumonia', 'NN'), ('.', '.')]\n",
      "JN--JNC--DJN-JN-CJJN-\n",
      "5 ['JN', 'JN', 'JN', 'JN', 'JJN']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = './50_reports(NLP_IR)/' + sorted_data_items[1]\n",
    "f = open(file, 'r')\n",
    "Lines = f.read().splitlines()\n",
    "str_Lines = ''.join(Lines)\n",
    "str_Lines = str_Lines.lower()\n",
    "# print(str_Lines)    \n",
    "sentences = sent_tokenize(str_Lines)\n",
    "# print(sentences)\n",
    "\n",
    "is_NN = lambda pos: pos[:2] == 'NN'\n",
    "is_JJ = lambda pos: pos[:2] == 'JJ'\n",
    "is_CC = lambda pos: pos[:2] == 'CC'\n",
    "is_DT = lambda pos: pos[:2] == 'DT'\n",
    "\n",
    "is_N = lambda pos: pos[:2] == 'N'\n",
    "is_J = lambda pos: pos[:2] == 'J'\n",
    "is_C = lambda pos: pos[:2] == 'C'\n",
    "is_D = lambda pos: pos[:2] == 'D'\n",
    "for idx, s in enumerate(sentences):\n",
    "    \n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(s)\n",
    "    print(str(idx) + \": \" + s.strip())\n",
    "    pos_tags = nltk.pos_tag(tokenized)\n",
    "    print(\"STEP1: word tokenizing with pos_tagging\\n\", pos_tags)\n",
    "    DCJN = list(map(lambda x: x[1][:1] if is_JJ(x[1]) or is_NN(x[1]) or is_CC(x[1]) or is_DT(x[1]) else '-', pos_tags))\n",
    "    DCJN = ''.join(DCJN)\n",
    "    print(DCJN)\n",
    "    JN = re.findall('[J]+[N]+', DCJN)\n",
    "    print(len(JN), JN)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: final report examination:  dx chest port line/tube plcmt 1 exam  indication:  ___ year old woman with likely ileus after cystectomy  // ngt placement confirmation      ngt placement confirmation  impression:   no previous images.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('final', 'JJ'), ('report', 'NN'), ('examination', 'NN'), (':', ':'), ('dx', 'NN'), ('chest', 'JJS'), ('port', 'NN'), ('line/tube', 'NN'), ('plcmt', 'VBD'), ('1', 'CD'), ('exam', 'JJ'), ('indication', 'NN'), (':', ':'), ('___', 'JJ'), ('year', 'NN'), ('old', 'JJ'), ('woman', 'NN'), ('with', 'IN'), ('likely', 'JJ'), ('ileus', 'NN'), ('after', 'IN'), ('cystectomy', 'NN'), ('//', 'JJ'), ('ngt', 'JJ'), ('placement', 'NN'), ('confirmation', 'NN'), ('ngt', 'FW'), ('placement', 'NN'), ('confirmation', 'NN'), ('impression', 'NN'), (':', ':'), ('no', 'DT'), ('previous', 'JJ'), ('images', 'NNS'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'J'), (1, 'N'), (2, 'N'), (4, 'N'), (5, 'J'), (6, 'N'), (7, 'N'), (10, 'J'), (11, 'N'), (13, 'J'), (14, 'N'), (15, 'J'), (16, 'N'), (18, 'J'), (19, 'N'), (21, 'N'), (22, 'J'), (23, 'J'), (24, 'N'), (25, 'N'), (27, 'N'), (28, 'N'), (29, 'N'), (31, 'D'), (32, 'J'), (33, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'J'), (1, 'N'), (2, 'N')], [(4, 'N'), (5, 'J'), (6, 'N'), (7, 'N')], [(10, 'J'), (11, 'N')], [(13, 'J'), (14, 'N'), (15, 'J'), (16, 'N')], [(18, 'J'), (19, 'N')], [(21, 'N'), (22, 'J'), (23, 'J'), (24, 'N'), (25, 'N')], [(27, 'N'), (28, 'N'), (29, 'N')], [(31, 'D'), (32, 'J'), (33, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(0, 'J'), (1, 'N'), (2, 'N')], [(4, 'N'), (5, 'J'), (6, 'N'), (7, 'N')], [(10, 'J'), (11, 'N')], [(13, 'J'), (14, 'N'), (15, 'J'), (16, 'N')], [(18, 'J'), (19, 'N')], [(21, 'N'), (22, 'J'), (23, 'J'), (24, 'N'), (25, 'N')], [(27, 'N'), (28, 'N'), (29, 'N')], [(31, 'D'), (32, 'J'), (33, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JNN', 'NJNN', 'JN', 'JNJN', 'JN', 'NJJNN', 'NNN', 'DJN']\n",
      "[['JNN'], ['JNN'], ['JN'], ['JN', 'JN'], ['JN'], ['JJNN'], [], ['JN']]\n",
      "STEP9: flat lists\n",
      " ['JNN', 'JNN', 'JN', 'JN', 'JN', 'JN', 'JJNN', 'JN']\n",
      "\n",
      "\n",
      "1: nasogastric tube extends to the mid body of the stomach, be for coiling on itself so that the tip lies close to the esophagogastric junction.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('nasogastric', 'JJ'), ('tube', 'NN'), ('extends', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('mid', 'NN'), ('body', 'NN'), ('of', 'IN'), ('the', 'DT'), ('stomach', 'NN'), (',', ','), ('be', 'VB'), ('for', 'IN'), ('coiling', 'VBG'), ('on', 'IN'), ('itself', 'PRP'), ('so', 'RB'), ('that', 'IN'), ('the', 'DT'), ('tip', 'NN'), ('lies', 'VBZ'), ('close', 'RB'), ('to', 'TO'), ('the', 'DT'), ('esophagogastric', 'JJ'), ('junction', 'NN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'J'), (1, 'N'), (4, 'D'), (5, 'N'), (6, 'N'), (8, 'D'), (9, 'N'), (18, 'D'), (19, 'N'), (23, 'D'), (24, 'J'), (25, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'J'), (1, 'N')], [(4, 'D'), (5, 'N'), (6, 'N')], [(8, 'D'), (9, 'N')], [(18, 'D'), (19, 'N')], [(23, 'D'), (24, 'J'), (25, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(0, 'J'), (1, 'N')], [(4, 'D'), (5, 'N'), (6, 'N')], [(8, 'D'), (9, 'N')], [(18, 'D'), (19, 'N')], [(23, 'D'), (24, 'J'), (25, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JN', 'DNN', 'DN', 'DN', 'DJN']\n",
      "[['JN'], [], [], [], ['JN']]\n",
      "STEP9: flat lists\n",
      " ['JN', 'JN']\n",
      "\n",
      "\n",
      "2: for more optimal positioning, the to would have to be pulled back almost 10 cm and then hopefully redirected toward the lower stomach.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('for', 'IN'), ('more', 'JJR'), ('optimal', 'JJ'), ('positioning', 'NN'), (',', ','), ('the', 'DT'), ('to', 'TO'), ('would', 'MD'), ('have', 'VB'), ('to', 'TO'), ('be', 'VB'), ('pulled', 'VBN'), ('back', 'RB'), ('almost', 'RB'), ('10', 'CD'), ('cm', 'NNS'), ('and', 'CC'), ('then', 'RB'), ('hopefully', 'RB'), ('redirected', 'VBN'), ('toward', 'IN'), ('the', 'DT'), ('lower', 'JJR'), ('stomach', 'NN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(1, 'J'), (2, 'J'), (3, 'N'), (5, 'D'), (15, 'N'), (16, 'C'), (21, 'D'), (22, 'J'), (23, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(1, 'J'), (2, 'J'), (3, 'N')], [(5, 'D')], [(15, 'N'), (16, 'C')], [(21, 'D'), (22, 'J'), (23, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(1, 'J'), (2, 'J'), (3, 'N')], [(15, 'N'), (16, 'C')], [(21, 'D'), (22, 'J'), (23, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JJN', 'NC', 'DJN']\n",
      "[['JJN'], [], ['JN']]\n",
      "STEP9: flat lists\n",
      " ['JJN', 'JN']\n",
      "\n",
      "\n",
      "3: cardiac silhouette is within normal limits and there is no vascular congestion, pleural effusion, or acute focal pneumonia.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('cardiac', 'JJ'), ('silhouette', 'NN'), ('is', 'VBZ'), ('within', 'IN'), ('normal', 'JJ'), ('limits', 'NNS'), ('and', 'CC'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('vascular', 'JJ'), ('congestion', 'NN'), (',', ','), ('pleural', 'JJ'), ('effusion', 'NN'), (',', ','), ('or', 'CC'), ('acute', 'JJ'), ('focal', 'JJ'), ('pneumonia', 'NN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'J'), (1, 'N'), (4, 'J'), (5, 'N'), (6, 'C'), (9, 'D'), (10, 'J'), (11, 'N'), (13, 'J'), (14, 'N'), (16, 'C'), (17, 'J'), (18, 'J'), (19, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'J'), (1, 'N')], [(4, 'J'), (5, 'N'), (6, 'C')], [(9, 'D'), (10, 'J'), (11, 'N')], [(13, 'J'), (14, 'N')], [(16, 'C'), (17, 'J'), (18, 'J'), (19, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(0, 'J'), (1, 'N')], [(4, 'J'), (5, 'N'), (6, 'C')], [(9, 'D'), (10, 'J'), (11, 'N')], [(13, 'J'), (14, 'N')], [(16, 'C'), (17, 'J'), (18, 'J'), (19, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JN', 'JNC', 'DJN', 'JN', 'CJJN']\n",
      "[['JN'], ['JN'], ['JN'], ['JN'], ['JJN']]\n",
      "STEP9: flat lists\n",
      " ['JN', 'JN', 'JN', 'JN', 'JJN']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = './50_reports(NLP_IR)/' + sorted_data_items[1]\n",
    "f = open(file, 'r')\n",
    "Lines = f.read().splitlines()\n",
    "str_Lines = ''.join(Lines)\n",
    "str_Lines = str_Lines.lower()\n",
    "# print(str_Lines)    \n",
    "sentences = sent_tokenize(str_Lines)\n",
    "# print(sentences)\n",
    "\n",
    "is_NN = lambda pos: pos[:2] == 'NN'\n",
    "is_JJ = lambda pos: pos[:2] == 'JJ'\n",
    "is_CC = lambda pos: pos[:2] == 'CC'\n",
    "is_DT = lambda pos: pos[:2] == 'DT'\n",
    "\n",
    "is_N = lambda pos: pos[:2] == 'N'\n",
    "is_J = lambda pos: pos[:2] == 'J'\n",
    "is_C = lambda pos: pos[:2] == 'C'\n",
    "is_D = lambda pos: pos[:2] == 'D'\n",
    "for idx, s in enumerate(sentences):\n",
    "    \n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(s)\n",
    "    print(str(idx) + \": \" + s.strip())\n",
    "    pos_tags = nltk.pos_tag(tokenized)\n",
    "    print(\"STEP1: word tokenizing with pos_tagging\\n\", pos_tags)\n",
    "    DorCorJorN = [(idx, pos[:1]) for idx, (word, pos) in enumerate(pos_tags) if is_JJ(pos) or is_NN(pos) or is_CC(pos) or is_DT(pos)]\n",
    "    print(\"STEP2: filtered by JJ or NN or CC or DT\\n\", DorCorJorN)\n",
    "    groupSeq_DorCorJorN = groupSequence(DorCorJorN)\n",
    "    print(\"STEP3: grouped by sequential items\\n\", groupSeq_DorCorJorN)\n",
    "    wordsMoreThan2_groupSeq_DorCorJorN = list(filter(lambda x: len(x) > 1, groupSeq_DorCorJorN))\n",
    "    print(\"STEP4: filtered by more then 2 words\\n\", wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "    joinPos_wordsMoreThan2_groupSeq_DorCorJorN = [''.join(list(map(lambda x: x[1], item))) for item in wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "    print(\"STEP5: join by POS prefix\\n\", joinPos_wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "    findallJN_joinPos_wordsMoreThan2_groupSeq_DorCorJorN = [re.findall('[J]+[N]+', item) for item in joinPos_wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "    print(findallJN_joinPos_wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "    flat_list = [item for sublist in findallJN_joinPos_wordsMoreThan2_groupSeq_DorCorJorN for item in sublist]\n",
    "    print(\"STEP9: flat lists\\n\", flat_list)\n",
    "#     for item in joinPos_wordsMoreThan2_groupSeq_DorCorJorN:\n",
    "# #         temp = re.findall('[N]*[D]?[J]+[N]+', item)\n",
    "#         temp = re.findall('[J]+[N]+', item)\n",
    "#         print(temp)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: final report indication:  ___-year-old woman with c. difficile colitis and increasing oxygen requirement.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('final', 'JJ'), ('report', 'NN'), ('indication', 'NN'), (':', ':'), ('___-year-old', 'JJ'), ('woman', 'NN'), ('with', 'IN'), ('c.', 'NN'), ('difficile', 'NN'), ('colitis', 'NN'), ('and', 'CC'), ('increasing', 'VBG'), ('oxygen', 'NN'), ('requirement', 'NN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'J'), (1, 'N'), (2, 'N'), (4, 'J'), (5, 'N'), (7, 'N'), (8, 'N'), (9, 'N'), (10, 'C'), (12, 'N'), (13, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'J'), (1, 'N'), (2, 'N')], [(4, 'J'), (5, 'N')], [(7, 'N'), (8, 'N'), (9, 'N'), (10, 'C')], [(12, 'N'), (13, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(0, 'J'), (1, 'N'), (2, 'N')], [(4, 'J'), (5, 'N')], [(7, 'N'), (8, 'N'), (9, 'N'), (10, 'C')], [(12, 'N'), (13, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JNN', 'JN', 'NNNC', 'NN']\n",
      "STEP6: split by CC\n",
      " [['JNN'], ['JN'], ['NNN', ''], ['NN']]\n",
      "STEP7: flat lists\n",
      " ['JNN', 'JN', 'NNN', '', 'NN']\n",
      "STEP8: split by DT\n",
      " [['JNN'], ['JN'], ['NNN'], [''], ['NN']]\n",
      "STEP9: flat lists\n",
      " ['JNN', 'JN', 'NNN', '', 'NN']\n",
      "STEP10: remove empty\n",
      " ['JNN', 'JN', 'NNN', 'NN']\n",
      "STEP11: find JJ+NN\n",
      " ['JNN', 'JN']\n",
      "\n",
      "\n",
      "1: findings:  a single portable semi-erect chest radiograph was obtained.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('findings', 'NNS'), (':', ':'), ('a', 'DT'), ('single', 'JJ'), ('portable', 'JJ'), ('semi-erect', 'JJ'), ('chest', 'NN'), ('radiograph', 'NN'), ('was', 'VBD'), ('obtained', 'VBN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'N'), (2, 'D'), (3, 'J'), (4, 'J'), (5, 'J'), (6, 'N'), (7, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'N')], [(2, 'D'), (3, 'J'), (4, 'J'), (5, 'J'), (6, 'N'), (7, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(2, 'D'), (3, 'J'), (4, 'J'), (5, 'J'), (6, 'N'), (7, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['DJJJNN']\n",
      "STEP6: split by CC\n",
      " [['DJJJNN']]\n",
      "STEP7: flat lists\n",
      " ['DJJJNN']\n",
      "STEP8: split by DT\n",
      " [['', 'JJJNN']]\n",
      "STEP9: flat lists\n",
      " ['', 'JJJNN']\n",
      "STEP10: remove empty\n",
      " ['JJJNN']\n",
      "STEP11: find JJ+NN\n",
      " ['JJJNN']\n",
      "\n",
      "\n",
      "2: small left and moderate layering right pleural effusions have increased in size since the preceding day's exam.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('small', 'JJ'), ('left', 'VBD'), ('and', 'CC'), ('moderate', 'JJ'), ('layering', 'NN'), ('right', 'JJ'), ('pleural', 'JJ'), ('effusions', 'NNS'), ('have', 'VBP'), ('increased', 'VBN'), ('in', 'IN'), ('size', 'NN'), ('since', 'IN'), ('the', 'DT'), ('preceding', 'VBG'), ('day', 'NN'), (\"'s\", 'POS'), ('exam', 'NN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'J'), (2, 'C'), (3, 'J'), (4, 'N'), (5, 'J'), (6, 'J'), (7, 'N'), (11, 'N'), (13, 'D'), (15, 'N'), (17, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'J')], [(2, 'C'), (3, 'J'), (4, 'N'), (5, 'J'), (6, 'J'), (7, 'N')], [(11, 'N')], [(13, 'D')], [(15, 'N')], [(17, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(2, 'C'), (3, 'J'), (4, 'N'), (5, 'J'), (6, 'J'), (7, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['CJNJJN']\n",
      "STEP6: split by CC\n",
      " [['', 'JNJJN']]\n",
      "STEP7: flat lists\n",
      " ['', 'JNJJN']\n",
      "STEP8: split by DT\n",
      " [[''], ['JNJJN']]\n",
      "STEP9: flat lists\n",
      " ['', 'JNJJN']\n",
      "STEP10: remove empty\n",
      " ['JNJJN']\n",
      "STEP11: find JJ+NN\n",
      " ['JNJJN']\n",
      "\n",
      "\n",
      "3: the right middle lobe pnemonia seen on recent ct is not clearly differentiated, but the right heart border is obscured.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('the', 'DT'), ('right', 'JJ'), ('middle', 'NN'), ('lobe', 'NN'), ('pnemonia', 'NN'), ('seen', 'VBN'), ('on', 'IN'), ('recent', 'JJ'), ('ct', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('clearly', 'RB'), ('differentiated', 'VBN'), (',', ','), ('but', 'CC'), ('the', 'DT'), ('right', 'JJ'), ('heart', 'NN'), ('border', 'NN'), ('is', 'VBZ'), ('obscured', 'VBN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'D'), (1, 'J'), (2, 'N'), (3, 'N'), (4, 'N'), (7, 'J'), (8, 'N'), (14, 'C'), (15, 'D'), (16, 'J'), (17, 'N'), (18, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'D'), (1, 'J'), (2, 'N'), (3, 'N'), (4, 'N')], [(7, 'J'), (8, 'N')], [(14, 'C'), (15, 'D'), (16, 'J'), (17, 'N'), (18, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(0, 'D'), (1, 'J'), (2, 'N'), (3, 'N'), (4, 'N')], [(7, 'J'), (8, 'N')], [(14, 'C'), (15, 'D'), (16, 'J'), (17, 'N'), (18, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['DJNNN', 'JN', 'CDJNN']\n",
      "STEP6: split by CC\n",
      " [['DJNNN'], ['JN'], ['', 'DJNN']]\n",
      "STEP7: flat lists\n",
      " ['DJNNN', 'JN', '', 'DJNN']\n",
      "STEP8: split by DT\n",
      " [['', 'JNNN'], ['JN'], [''], ['', 'JNN']]\n",
      "STEP9: flat lists\n",
      " ['', 'JNNN', 'JN', '', '', 'JNN']\n",
      "STEP10: remove empty\n",
      " ['JNNN', 'JN', 'JNN']\n",
      "STEP11: find JJ+NN\n",
      " ['JNNN', 'JN', 'JNN']\n",
      "\n",
      "\n",
      "4: left basilar atelectasis is stable.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('left', 'VBD'), ('basilar', 'JJ'), ('atelectasis', 'NN'), ('is', 'VBZ'), ('stable', 'JJ'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(1, 'J'), (2, 'N'), (4, 'J')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(1, 'J'), (2, 'N')], [(4, 'J')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(1, 'J'), (2, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JN']\n",
      "STEP6: split by CC\n",
      " [['JN']]\n",
      "STEP7: flat lists\n",
      " ['JN']\n",
      "STEP8: split by DT\n",
      " [['JN']]\n",
      "STEP9: flat lists\n",
      " ['JN']\n",
      "STEP10: remove empty\n",
      " ['JN']\n",
      "STEP11: find JJ+NN\n",
      " ['JN']\n",
      "\n",
      "\n",
      "5: no new focal consolidation or pneumothorax is present.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('no', 'DT'), ('new', 'JJ'), ('focal', 'JJ'), ('consolidation', 'NN'), ('or', 'CC'), ('pneumothorax', 'NN'), ('is', 'VBZ'), ('present', 'JJ'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'D'), (1, 'J'), (2, 'J'), (3, 'N'), (4, 'C'), (5, 'N'), (7, 'J')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'D'), (1, 'J'), (2, 'J'), (3, 'N'), (4, 'C'), (5, 'N')], [(7, 'J')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(0, 'D'), (1, 'J'), (2, 'J'), (3, 'N'), (4, 'C'), (5, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['DJJNCN']\n",
      "STEP6: split by CC\n",
      " [['DJJN', 'N']]\n",
      "STEP7: flat lists\n",
      " ['DJJN', 'N']\n",
      "STEP8: split by DT\n",
      " [['', 'JJN'], ['N']]\n",
      "STEP9: flat lists\n",
      " ['', 'JJN', 'N']\n",
      "STEP10: remove empty\n",
      " ['JJN', 'N']\n",
      "STEP11: find JJ+NN\n",
      " ['JJN']\n",
      "\n",
      "\n",
      "6: hila remain indistinct.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('hila', 'NN'), ('remain', 'VBP'), ('indistinct', 'JJ'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'N'), (2, 'J')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'N')], [(2, 'J')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " []\n",
      "STEP5: join by POS prefix\n",
      " []\n",
      "STEP6: split by CC\n",
      " []\n",
      "STEP7: flat lists\n",
      " []\n",
      "STEP8: split by DT\n",
      " []\n",
      "STEP9: flat lists\n",
      " []\n",
      "STEP10: remove empty\n",
      " []\n",
      "STEP11: find JJ+NN\n",
      " []\n",
      "\n",
      "\n",
      "7: a left-sided picc line tip remains in the upper svc.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('a', 'DT'), ('left-sided', 'JJ'), ('picc', 'NN'), ('line', 'NN'), ('tip', 'NN'), ('remains', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('upper', 'JJ'), ('svc', 'NN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'D'), (1, 'J'), (2, 'N'), (3, 'N'), (4, 'N'), (7, 'D'), (8, 'J'), (9, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'D'), (1, 'J'), (2, 'N'), (3, 'N'), (4, 'N')], [(7, 'D'), (8, 'J'), (9, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(0, 'D'), (1, 'J'), (2, 'N'), (3, 'N'), (4, 'N')], [(7, 'D'), (8, 'J'), (9, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['DJNNN', 'DJN']\n",
      "STEP6: split by CC\n",
      " [['DJNNN'], ['DJN']]\n",
      "STEP7: flat lists\n",
      " ['DJNNN', 'DJN']\n",
      "STEP8: split by DT\n",
      " [['', 'JNNN'], ['', 'JN']]\n",
      "STEP9: flat lists\n",
      " ['', 'JNNN', '', 'JN']\n",
      "STEP10: remove empty\n",
      " ['JNNN', 'JN']\n",
      "STEP11: find JJ+NN\n",
      " ['JNNN', 'JN']\n",
      "\n",
      "\n",
      "8: impression:  interval increase inmoderate to large right and small left pleural effusions.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('impression', 'NN'), (':', ':'), ('interval', 'JJ'), ('increase', 'NN'), ('inmoderate', 'NN'), ('to', 'TO'), ('large', 'JJ'), ('right', 'NN'), ('and', 'CC'), ('small', 'JJ'), ('left', 'VBD'), ('pleural', 'JJ'), ('effusions', 'NNS'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'N'), (2, 'J'), (3, 'N'), (4, 'N'), (6, 'J'), (7, 'N'), (8, 'C'), (9, 'J'), (11, 'J'), (12, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'N')], [(2, 'J'), (3, 'N'), (4, 'N')], [(6, 'J'), (7, 'N'), (8, 'C'), (9, 'J')], [(11, 'J'), (12, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(2, 'J'), (3, 'N'), (4, 'N')], [(6, 'J'), (7, 'N'), (8, 'C'), (9, 'J')], [(11, 'J'), (12, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JNN', 'JNCJ', 'JN']\n",
      "STEP6: split by CC\n",
      " [['JNN'], ['JN', 'J'], ['JN']]\n",
      "STEP7: flat lists\n",
      " ['JNN', 'JN', 'J', 'JN']\n",
      "STEP8: split by DT\n",
      " [['JNN'], ['JN'], ['J'], ['JN']]\n",
      "STEP9: flat lists\n",
      " ['JNN', 'JN', 'J', 'JN']\n",
      "STEP10: remove empty\n",
      " ['JNN', 'JN', 'J', 'JN']\n",
      "STEP11: find JJ+NN\n",
      " ['JNN', 'JN', 'JN']\n",
      "\n",
      "\n",
      "9: persistent right basilar pneumonia.\n",
      "STEP1: word tokenizing with pos_tagging\n",
      " [('persistent', 'JJ'), ('right', 'RB'), ('basilar', 'JJ'), ('pneumonia', 'NN'), ('.', '.')]\n",
      "STEP2: filtered by JJ or NN or CC or DT\n",
      " [(0, 'J'), (2, 'J'), (3, 'N')]\n",
      "STEP3: grouped by sequential items\n",
      " [[(0, 'J')], [(2, 'J'), (3, 'N')]]\n",
      "STEP4: filtered by more then 2 words\n",
      " [[(2, 'J'), (3, 'N')]]\n",
      "STEP5: join by POS prefix\n",
      " ['JN']\n",
      "STEP6: split by CC\n",
      " [['JN']]\n",
      "STEP7: flat lists\n",
      " ['JN']\n",
      "STEP8: split by DT\n",
      " [['JN']]\n",
      "STEP9: flat lists\n",
      " ['JN']\n",
      "STEP10: remove empty\n",
      " ['JN']\n",
      "STEP11: find JJ+NN\n",
      " ['JN']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = './50_reports(NLP_IR)/' + sorted_data_items[0]\n",
    "f = open(file, 'r')\n",
    "Lines = f.read().splitlines()\n",
    "str_Lines = ''.join(Lines)\n",
    "str_Lines = str_Lines.lower()\n",
    "# print(str_Lines)    \n",
    "sentences = sent_tokenize(str_Lines)\n",
    "# print(sentences)\n",
    "\n",
    "is_NN = lambda pos: pos[:2] == 'NN'\n",
    "is_JJ = lambda pos: pos[:2] == 'JJ'\n",
    "is_CC = lambda pos: pos[:2] == 'CC'\n",
    "is_DT = lambda pos: pos[:2] == 'DT'\n",
    "\n",
    "is_N = lambda pos: pos[:2] == 'N'\n",
    "is_J = lambda pos: pos[:2] == 'J'\n",
    "is_C = lambda pos: pos[:2] == 'C'\n",
    "is_D = lambda pos: pos[:2] == 'D'\n",
    "for idx, s in enumerate(sentences):\n",
    "    \n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(s)\n",
    "    print(str(idx) + \": \" + s.strip())\n",
    "    pos_tags = nltk.pos_tag(tokenized)\n",
    "    print(\"STEP1: word tokenizing with pos_tagging\\n\", pos_tags)\n",
    "    DorCorJorN = [(idx, pos[:1]) for idx, (word, pos) in enumerate(pos_tags) if is_JJ(pos) or is_NN(pos) or is_CC(pos) or is_DT(pos)]\n",
    "    print(\"STEP2: filtered by JJ or NN or CC or DT\\n\", DorCorJorN)\n",
    "    groupSeq_DorCorJorN = groupSequence(DorCorJorN)\n",
    "    print(\"STEP3: grouped by sequential items\\n\", groupSeq_DorCorJorN)\n",
    "#     [print([len(item), item]) for item in groupSeq_DorCorJorN]\n",
    "    wordsMoreThan2_groupSeq_DorCorJorN = list(filter(lambda x: len(x) > 1, groupSeq_DorCorJorN))\n",
    "    print(\"STEP4: filtered by more then 2 words\\n\", wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "#     [print([len(item), item]) for item in wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "#     rejectNN_wordsMoreThan2_groupSeq_DorCorJorN = list(filter(lambda x: not (is_N(x[0][1])), wordsMoreThan2_groupSeq_DorCorJorN))\n",
    "#     [print(item) for item in rejectNN_wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "    joinPos_wordsMoreThan2_groupSeq_DorCorJorN = [''.join(list(map(lambda x: x[1], item))) for item in wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "    print(\"STEP5: join by POS prefix\\n\", joinPos_wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "#     [print(item) for item in joinPos_wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "#     print(\"STEP6: \", joinPos_wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "#     splitC_joinPos_wordsMoreThan2_groupSeq_DorCorJorN = [re.findall('[N]*[D]?[J]+[N]+', joinPos_wordsMoreThan2_groupSeq_DorCorJorN) for item in joinPos_wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "    splitC_joinPos_wordsMoreThan2_groupSeq_DorCorJorN = [item.split('C') for item in joinPos_wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "    print(\"STEP6: split by CC\\n\", splitC_joinPos_wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "    flat_list = [item for sublist in splitC_joinPos_wordsMoreThan2_groupSeq_DorCorJorN for item in sublist]\n",
    "    print(\"STEP7: flat lists\\n\", flat_list)\n",
    "    splitD_flat_list = [item.split('D') for item in flat_list]\n",
    "    print(\"STEP8: split by DT\\n\", splitD_flat_list)\n",
    "    flat_list = [item for sublist in splitD_flat_list for item in sublist]\n",
    "    print(\"STEP9: flat lists\\n\", flat_list)\n",
    "    flat_list = list(filter(lambda x: x != '', flat_list))\n",
    "    print(\"STEP10: remove empty\\n\", flat_list)\n",
    "#     final_list = re.split('([D]?[J]+[N]+)', flat_list)\n",
    "    p = re.compile('[N]*[D]?[J]+[N]+')\n",
    "#     l1 = [\"AB.22-01-01\", \"AB.33-01-44\", \"--4\", \"AA.44--05\", \"--\"]\n",
    "#     l2 = [ s for s in l1 if p.match(s) ]\n",
    "    final_list = [s for s in flat_list if p.match(s)]\n",
    "    print(\"STEP11: find JJ+NN\\n\", final_list)\n",
    "#     print(joinPos_rejectNN_wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "#     splitC_joinPos_rejectNN_wordsMoreThan2_groupSeq_DorCorJorN = [item.split('C') for item in joinPos_rejectNN_wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "#     print(splitC_joinPos_rejectNN_wordsMoreThan2_groupSeq_DorCorJorN)\n",
    "#     [print(pos) for pos in list(map(lambda x: x, rejectNN_wordsMoreThan2_groupSeq_DorCorJorN))]\n",
    "#     [print([len(item), item]) for item in rejectIdx_rejectNN_wordsMoreThan2_groupSeq_DorCorJorN]\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for word, pos in pos_tags:\n",
    "        if pos == \"JJ\":\n",
    "            print(word)\n",
    "#     nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_NN(pos)]\n",
    "#     print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in sorted_data_items:\n",
    "    f = open(file, 'r') \n",
    "    Lines = f.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
